{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description B-3: Gradient descent (reference)\n",
    "\n",
    "Suppose we are given a function $f(x, y)$ in two real variables. Assume that this function is convex. A simple example would be the function\n",
    "$$f (x, y) = x2 + y2$$\n",
    "which has a unique minimum at $(x, y) = (0, 0)$. Imagine that the function $z = f (x, y)$ describes a landscape (with $x$, $y$ in the plane, and $z$ as the vertical\n",
    "coordinate). The gradient vector\n",
    "$$\\Delta{f(x, y)} = (fx, fy)$$\n",
    "describes the direction in which the function $f$ increases the most at the point $(x, y)$, (or the direction of maximum decrease for $−\\Delta{f(x, y)}$).\n",
    "\n",
    "- Consider this fact for the simple function $f(x, y) = x^2 + y^2$. Sketch the *contour lines* $x^2 + y^2 =$ const and verify that the gradient vector $\\Delta{f}$ is orthogonal to the contour lines at each point (sketch!). This property forms the basis for the gradient descent method, see also Exercise 4 below.\n",
    "    \n",
    "    To find a minimum in general, we proceed iteratively, starting from an initial approximation $(x0, y0)$:\n",
    "    $$(x_1, y_1) := (x_0, y_0) − \\gamma \\Delta f(x_0, y_0)$$\n",
    "\n",
    "    Here, $\\gamma > 0$ is a parameter that needs to be chosen appropriately. Choosing $\\gamma = 1$ might cause the iteration step to ’overshoot’.\n",
    "    \n",
    "    In this case, one might choose $\\gamma = 1/2$ and repeat the process, etc.\n",
    "\n",
    "- Implement and test this strategy for the model problem $f(x, y) = x^2 + y^2$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise B-4: Justification of the approach in Exercise B-3\n",
    "\n",
    "The equation $f(x, y) = \\text{const}$. describes a curve in the plane implicitly. Under suitable conditions, such a curve can also be described explicitly, for example in the form $(x(t), y(t))$ with a parameter $t$. One can think of this as a trajectory of an object at time $t$. Then $(x′(t), y′(t))^T$ is the tangent vector (specifically: velocity vector) to the curve at the point $(x(t), y(t))^T$ . \n",
    "\n",
    "Refer to Exercise 3.a):\n",
    "\n",
    "Prove by using the chain rule that the gradient $(fx, fy)$ is indeed orthogonal to the tangent vector $(x′, y′)$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
