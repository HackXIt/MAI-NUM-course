{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description B-3: Gradient descent (reference)\n",
    "\n",
    "Suppose we are given a function $f(x, y)$ in two real variables. Assume that this function is convex. A simple example would be the function\n",
    "$$f (x, y) = x2 + y2$$\n",
    "which has a unique minimum at $(x, y) = (0, 0)$. Imagine that the function $z = f (x, y)$ describes a landscape (with $x$, $y$ in the plane, and $z$ as the vertical\n",
    "coordinate). The gradient vector\n",
    "$$\\Delta{f(x, y)} = (fx, fy)$$\n",
    "describes the direction in which the function $f$ increases the most at the point $(x, y)$, (or the direction of maximum decrease for $−\\Delta{f(x, y)}$).\n",
    "\n",
    "- Consider this fact for the simple function $f(x, y) = x^2 + y^2$. Sketch the *contour lines* $x^2 + y^2 =$ const and verify that the gradient vector $\\Delta{f}$ is orthogonal to the contour lines at each point (sketch!). This property forms the basis for the gradient descent method, see also Exercise 4 below.\n",
    "    \n",
    "    To find a minimum in general, we proceed iteratively, starting from an initial approximation $(x0, y0)$:\n",
    "    $$(x_1, y_1) := (x_0, y_0) − \\gamma \\Delta f(x_0, y_0)$$\n",
    "\n",
    "    Here, $\\gamma > 0$ is a parameter that needs to be chosen appropriately. Choosing $\\gamma = 1$ might cause the iteration step to ’overshoot’.\n",
    "    \n",
    "    In this case, one might choose $\\gamma = 1/2$ and repeat the process, etc.\n",
    "\n",
    "- Implement and test this strategy for the model problem $f(x, y) = x^2 + y^2$.\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise B-5: Generalization of Task 3 for the case $f=f(x,y,z)$\n",
    "\n",
    "We again choose the simplest example: $f(x, y, z) = x^2 + y^2 + z^2$."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
